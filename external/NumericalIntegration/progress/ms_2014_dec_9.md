# Unit Tests and Node Weight Required Digits Update

I've relaxed the pass criteria so that when the presence of roundoff error is detected but the end value meets successful criteria the test is flagged as passing.  This allows 3 of the 4 tests (Sine, LogPow, and InfiniteInterval) to pass for float, double, and the setprecision(x) values in the files.  The peak unit test does not pass for anything except setprecision(6), it fails float, double, and long double.  The LogPow test fails only the long double and this distinction remains a mystery.

Next, through a series of checks in the unit tests I discovered that the computed node/weight values did not match the L.W. Fullerton values, so I spent some time checking and found that the computation of the nodes/weights requires roughly 4 times the precision of the desired significant figure output for the nodes/weights values.

## Discussion 

This subtlety with the nodes weights values was easy to overlook, as initially we had used L.W. Fullerton's values to check our calculations, then once the Piessens and Laurie codes agreed we had only compared them against each other.  This interplay unfortunately had previously made it appear as though a greater number digits was needed in the nodes/weights, however, this is not the case.  The nodes/weights must simply must be calculated correctly and precisely. Previously, the values were not being computed accurately because we were arbitrarily not enforcing that the calculations be carried at enough precision.  Effectively, to generate accurate nodes/weights the setprecision value must be greater than 120 to generate accuracy at 33 digits.

This contradicts the previous assertion I had made about the requirement to carry greater precision in the nodes/weights, so it is worth underscoring the fact that the mistake was not disagreement between Piessens/Laurie, but between our precision in the Piessens/Laurie calculations such that they matched or exceeded the accuracy of the Fullerton numbers... A convergence analysis is probably in order to see how the values asymptote in relation to the precision carried.  I am willing to venture a guess that this might have been done by the originaly QUADPACK group and this was the reason for settling on 33 digits of precision.

Testing the Sine, LogPow, and InfiniteInterval tests revealed no differences when using tabulated node/weight values beyond 33 digits provided those values are precise to 33 digits.  It is fortunate that we have L.W. Fullerton's values to compare to the first 6 rulesets, or this occurrence might have remained hidden as previous.

This new information points to an idea that it might be more important to have a greater number of rulesets than it is to have greater precision in the nodes/weights.  It also implies that the requirement for on-the-fly calculation is only necessary for rulesets that do not yet exist rather than to produce more accurate nodes/weights.  

We might be able to now choose to circumvent the need for on-the-fly calculation entirely by populating a greater number of rules pre-emptivly.

## Next steps

The Peak test still does not pass adequately.  It previously passed as double precision, prior to templating, and I have scoured our GPC repo for the difference but cannot discover any.  I have moved carefully through each unit test as well as Integrator.h to make sure that typecasting was purposefully carried out to Scalar type for any value with a decimal point, and although this might be good pracice, it has not improved the performance of the peak test.

I will continue investigating the Peak test and the long double failure of the LogPow test.